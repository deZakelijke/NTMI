{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 \n",
    "**Due: Friday 17th**\n",
    "\n",
    "This exercise contains some theoretical problems (10 points) and some programming ones (14 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1                                               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) What is the equation for $ P(\\text{See the small dog on the sidewalk})$ ?\n",
    "\n",
    "(i) Under the bigram language model?\n",
    "\n",
    "(ii) Under the trigram language model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i)\n",
    "\n",
    "$P(See the small dog on the sidewalk) = P(See)*P(the|See)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Consider a corpus with three sentences:\n",
    "\n",
    "$<s> \\text{I am Sam} </s>$\n",
    "\n",
    "$<s> \\text{Sam I am} </s>$\n",
    "\n",
    "$<s> \\text{I do not like green eggs and ham} </s>$\n",
    "\n",
    "Compute the following bigram probabilities:\n",
    "$P(I | <s>)=$ \n",
    "\n",
    "$P(</s> | Sam) =$\n",
    "\n",
    "$P(Sam | <s>)= $\n",
    "\n",
    "$P(am | I)= $\n",
    "\n",
    "$P(Sam | am) =$\n",
    "\n",
    "$P(do | I) =$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) \n",
    "A student in NTMI would like to build a simple spelling corrector to solve the problem of _their_ vs. _there_ in English. The model would take as input sentences like\n",
    "\n",
    "_He saw their cat on the street._\n",
    "\n",
    "_He saw their was a cat on the street._\n",
    "\n",
    "and for each instance of _there_ or _their_, predict whether the true spelling should be _there_ or _their_. So, the model should predict _their_ for sentence 1 above, and _there_ for sentence 2 above. For the second example the model would correct the spelling mistake in the sentence.\n",
    "\n",
    "The student decides to use a language model for the task. Given a language model $p(w_1...w_n)$, he returns the spelling that gives the highest probability under the language model. So for example for the second sentence he implements the rule:\n",
    "\n",
    "    If p(He saw their was a cat on the street) > p(He saw there was a cat on the street)\n",
    "    Then Return their\n",
    "    Else Return there\n",
    "\n",
    "(i) The first language model the student tries is a uni-gram model. Assume that the student uses MLE (i.e. $p(w_i) = count(w_i)/N$, where $count(w_i)$ is the number of times a word occurs in a corpus and $N$ is the total number of words in the corpus. Also assume that for every word $v$ in the vocabulary , $count(v) > 0$). Write the general equation for sentence probability under the unigram model, i.e. $p(w_1 . . . w_n)$. Does this seem like a good solution to the _their_ vs. _there_ problem? Justify your answer.\n",
    "\n",
    "(ii) Next, the student tries a bi-gram language model. Write the general equation for sentence probability under the bi-gram model. Why might this model be better than the model in the previous question?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Give three examples in English or Dutch where English/Dutch grammar indicates that the independence assumption of a tri-gram model is very clearly violated.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "Use the Penn Treebank data (sec02-21.raw) from the previous exercise. To start with, consider the corpus to be one continuous paragraph (i.e., ignore the line-breaks that split the corpus into one sentence per line)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Construct a table of all bi-grams, tri-grams and 4-grams, i.e. word sequences of length 2, 3, and 4 in this corpus, together with the number of times each sequence appears in the corpus (the n-gram corpus frequency of the sequence). In this case, do not perform any text processing on the corpus. So 'The' $\\neq$ 'the'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Output the 10 most frequent sequences, together with their frequencies and in decreasing frequency order, for $n=2$, $n=3$, $n=4$ ($n$ is the order of the n-gram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Output the sum of all frequencies of all sequences for $n=2$, $n=3$, $n=4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Modify your program to add START/STOP symbols at the beginning and end of each sentence, and reconstruct the tables in (a). Notice that by adding the START/STOP symbols, the frequencies may have changed. Output the 10 most frequent bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(e) Give the probabilities of the first 5 sentences in the *training* corpus, according to the bi-gram model in (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) You are also provided with another file called sec00.raw. Section 00 of the Penn Treebank is typically used as **development** data. (Just FYI, sections 02-21, i.e. the data you were using so far, are by convention used as **training** data in the NLP community)\n",
    "\n",
    "How many unseen words are present in the development data (i.e., words that have not occurred in the training data)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Find the first 5 sentences in the *development* corpus that do not contain any unseen words. Output the probabilities of these sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
