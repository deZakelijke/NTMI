{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 \n",
    "\n",
    "\n",
    "#### Total Points 21. Due Friday 24th\n",
    "\n",
    "Points for each question are indicated within double brackets and boldface next to each. \n",
    "\n",
    "Use the POS tagged training and development data (from BB) for answering a, b, and c below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) A word may have several part-of-speech tags, for example the word 'record' can be a noun or a verb. How many words do have more than one POS tag? What are the 10 most frequent combinations of POS tags? **((1))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) What are the three most common kind of unseen word (their POS tags)? **((1))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Build a tri-gram language model over tags, from the PTB training data, using code from previous exercise. **((3))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Consider the following sentence:  **((1))**\n",
    "\n",
    "__The young guard the lifeboats__\n",
    "\n",
    "Give the POS tags of all words in the sentence, such that the sentence is a grammatical sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e)\n",
    "\n",
    "Use a version of _bigram tagging_ as described in the lecture to assign tags to the above sentence, using the tags DT, N, V, ADJ, based on the following frequency data. (Rows correspond to potential POS tags for the word in question; columns correspond to the POS tag of the preceding word.) Assume 'the' and 'lifeboats' can only be tagged as DT and N respectively. **((3))**\n",
    "\n",
    "\n",
    "|young | DT | N | V  |Adj\n",
    "|---     |:--- |:---|:--- |:---|\n",
    "|N       | 8  | 2 | 3  | 2 |\n",
    "|V       | 0  | 0 | 0  | 0 |\n",
    "|Adj     | 34 | 5 | 13 | 17|\n",
    "\n",
    "\n",
    "\n",
    "|guard | DT | N | V | Adj |\n",
    "|----|----|---|---|----|\n",
    "|N | 102 | 45 | 15 | 86 |\n",
    "|V | 0 | 11 | 4 | 4 |\n",
    "|Adj | 0 | 0 | 0 | 0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) This question does not require coding, and can be done by hand. \n",
    "\n",
    "Suppose we want to train an HMM tagger for the task of Named Entity Recognition (NER). We are interested in only two kinds of named entities: persons (PER) and organizations (ORG), which include corporate and political entities. We have the following training data  (Slightly modified entry for Barack Obama from Wikipedia)\n",
    "\n",
    "Barack/PER Hussein/PER Obama/PER II/PER ( born August 4 , 1961 ) is an American politician who is the 44th and current President of the United/ORG States/ORG . \n",
    "\n",
    "He is the first African American to hold the office and the first president born outside the continental United/ORG States/ORG . \n",
    "\n",
    "Born in Honolulu , Hawaii , Obama/PER is a graduate of Columbia/ORG University/ORG and Harvard/ORG Law/ORG School/ORG , where he was president of the Harvard/ORG Law/ORG Review/ORG . \n",
    "\n",
    "Obama/PER was a community organizer in Chicago before earning his law degree . He worked as a civil rights attorney and taught constitutional law at the University/ORG of Chicago/ORG Law/ORG School/ORG between 1992 and 2004 . \n",
    "\n",
    "\n",
    "_In this data we show only the tags for the words belonging to the person and organization categories. Assume all other words (including punctuation) have the tag OTH, which is not shown. There are 112 tokens and 69 types in the text._  \n",
    "\n",
    "(This question adapted from S Goldwater)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Give the transition probability matrix estimated from this training data using maximum-likelihood estimation. Don't forget to include beginning and end of sentence markers. **((2))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii) Now do the same but using add-one smoothing. Assume that all sentences must contain at least one word (i.e., $P(</s> | <s>)$ is zero even in the smoothed model). **((2))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii) Again using add-one smoothing, what are the estimates for $P(Obama|PER)$\n",
    "and  $P(Obama|ORG)$? **((1))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g)\n",
    "Hand-simulate the __Viterbi__ algorithm to tag the sentence **The young guard the lifeboats**, using the following transition and emission probabilities.  That is, hand-simulate the Viterbi algorithm in order to compute the highest probability tag sequence for the given sentence. Fill in the cells in a table, where cell $[ j,t]$ should contain the Viterbi value for state $j$ at time $t$. Include explicit backtrace pointers in your Viterbi matrix. (Note that in the transition matrix, rows represent the previous state and columns represent the next state) **((4))**\n",
    "\n",
    "\n",
    "|\t| DT | N | V | Adj |\n",
    "|---|----|---|---|---|    \n",
    "|$<s>$ | 0.4 | 0.3 | 0.1 | 0.2 |\n",
    "|DT | 0 | 0.6 | 0 | 0.4 |\n",
    "|N | 0.05 | 0.3 | 0.4 | 0.25 |\n",
    "|V | 0.4 | 0.3 | 0.1 | 0.2 |\n",
    "|Adj | 0.1 | 0.5 | 0.2 | 0.2 |\n",
    "\n",
    "\n",
    "\n",
    " | | lifeboats | guard | young | the |\n",
    " |---|-----|----|------|---|\n",
    " | DT | 0 | 0 | 0 | 0.5 |\n",
    " | N | 0.2 | 0.3 | 0.2 | 0 |\n",
    " | V | 0 | 0.1 | 0 | 0 |\n",
    " | Adj | 0 | 0 | 0.4 | 0 |\n",
    "\n",
    "  \n",
    "\n",
    "Also calculate the joint probability of the sentence with this tag sequence. (Note that in this example, we are ignoring end of sentence markers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h)\n",
    "Now, implement the Viterbi algorithm in code. Make sure that your code produces the same tags and probability as your hand-simulation. **((3))**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
